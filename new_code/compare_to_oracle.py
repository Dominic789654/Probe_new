import json
import argparse
from collections import defaultdict
import os
import re # Import the regular expression module
import pandas as pd

def parse_args():
    """Parses command-line arguments for the script."""
    parser = argparse.ArgumentParser(description="Compare a method's selections against the Oracle selections.")
    parser.add_argument("--method_file", type=str, required=True,
                        help="Path to the JSONL file containing selections from your method.")
    parser.add_argument("--oracle_file", type=str, required=True,
                        help="Path to the JSONL file containing Oracle selections (generated by calculate_oracle_performance.py).")
    # parser.add_argument("--output_file", type=str, required=False,
    #                     help="Optional path to save a detailed comparison report as a CSV file.")
    return parser.parse_args()

def load_jsonl_as_dict(path):
    """Loads a JSONL file into a dictionary keyed by the 'problem' field."""
    data = {}
    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            item = json.loads(line)
            if 'problem' in item:
                data[item['problem']] = item
            else:
                print(f"Warning: Found an entry without a 'problem' key in {path}. Skipping.")
    return data

def main():
    """Main function to compare method vs. oracle performance."""
    args = parse_args()

    # --- Step 1: Load data ---
    print(f"Loading method selections from: {args.method_file}")
    method_selections = load_jsonl_as_dict(args.method_file)
    
    print(f"Loading Oracle selections from: {args.oracle_file}")
    oracle_selections = load_jsonl_as_dict(args.oracle_file)

    if not method_selections or not oracle_selections:
        print("One or both input files are empty. Exiting.")
        return
        
    print(f"Found {len(method_selections)} selections for the method and {len(oracle_selections)} for the Oracle.")

    # --- Step 2: Find common problems and initialize stats ---
    common_problems = sorted(list(set(method_selections.keys()) & set(oracle_selections.keys())))
    print(f"Found {len(common_problems)} common problems to compare.")

    if not common_problems:
        print("No common problems found between the two files. Exiting.")
        return

    # Overall stats
    method_correct_count = 0
    oracle_correct_count = 0
    method_total_tokens = 0
    oracle_total_tokens = 0

    # Head-to-head stats
    both_correct = 0
    oracle_only_correct = 0
    method_only_correct = 0 # Should be 0 if oracle has access to all runs
    both_failed = 0
    
    # Strategy and token stats
    same_strategy_count = 0
    token_diff_when_both_correct = 0

    # Difficulty stats
    difficulty_stats = {
        'Easy': {'total': 0, 'agreed': 0},
        'Normal': {'total': 0, 'agreed': 0},
        'Hard': {'total': 0, 'agreed': 0},
        'Unknown': {'total': 0, 'agreed': 0}
    }

    # --- Step 3: Iterate and compare selections for each problem ---
    for problem in common_problems:
        method_run = method_selections[problem]
        oracle_run = oracle_selections[problem]

        is_method_correct = method_run.get('xverify_evaluation', {}).get('Correctness') == 'Correct'
        is_oracle_correct = oracle_run.get('xverify_evaluation', {}).get('Correctness') == 'Correct'
        
        # Update overall stats
        if is_method_correct:
            method_correct_count += 1
        if is_oracle_correct:
            oracle_correct_count += 1
            
        method_total_tokens += method_run.get('total_generated_tokens', 0)
        oracle_total_tokens += oracle_run.get('total_generated_tokens', 0)

        # Update head-to-head stats
        if is_method_correct and is_oracle_correct:
            both_correct += 1
            token_diff_when_both_correct += (method_run.get('total_generated_tokens', 0) - oracle_run.get('total_generated_tokens', 0))
        elif not is_method_correct and is_oracle_correct:
            oracle_only_correct += 1
        elif is_method_correct and not is_oracle_correct:
            method_only_correct += 1
        else: # Both failed
            both_failed += 1
            
        # --- Update strategy and difficulty stats ---
        # Get difficulty from the 'strategy' field
        difficulty = method_run.get('strategy', 'Unknown').capitalize()
        if difficulty not in difficulty_stats:
            difficulty = 'Unknown' # Handle potential other values like 'simple' etc.
        
        difficulty_stats[difficulty]['total'] += 1

        # Update strategy stats
        if method_run['strategy'] == oracle_run['strategy']:
            same_strategy_count += 1
            difficulty_stats[difficulty]['agreed'] += 1

    # --- Step 4: Calculate and print the comparison report ---
    num_problems = len(common_problems)
    
    # Overall Performance
    method_accuracy = (method_correct_count / num_problems) * 100
    oracle_accuracy = (oracle_correct_count / num_problems) * 100
    method_avg_tokens = method_total_tokens / num_problems
    oracle_avg_tokens = oracle_total_tokens / num_problems

    print("\n--- Comparison Report: Your Method vs. Oracle ---")
    print(f"\nTotal Problems Compared: {num_problems}")
    print("\n--- Overall Performance ---")
    header = f"| {'Metric':<18} | {'Your Method':<18} | {'Oracle':<18} |"
    print(header)
    print("-" * len(header))
    print(f"| {'Problems Solved':<18} | {method_correct_count:<18} | {oracle_correct_count:<18} |")
    print(f"| {'Accuracy (Pass@1)':<18} | {method_accuracy:<17.2f}% | {oracle_accuracy:<17.2f}% |")
    print(f"| {'Avg. Tokens':<18} | {method_avg_tokens:<18.2f} | {oracle_avg_tokens:<18.2f} |")

    # Head-to-Head Comparison
    print("\n--- Head-to-Head Comparison ---")
    print(f"- Problems where BOTH were CORRECT: {both_correct}")
    if both_correct > 0:
        avg_token_diff = token_diff_when_both_correct / both_correct
        print(f"  - On these, your method used an average of {avg_token_diff:.2f} more tokens per problem.")
    print(f"- Problems where Oracle was CORRECT but Your Method FAILED: {oracle_only_correct}")
    if method_only_correct > 0:
        print(f"- Problems where Your Method was CORRECT but Oracle FAILED: {method_only_correct} (Note: This is unexpected)")
    print(f"- Problems where BOTH FAILED: {both_failed}")
    
    # Strategy Selection Comparison
    strategy_agreement_perc = (same_strategy_count / num_problems) * 100 if num_problems > 0 else 0
    print("\n--- Strategy Selection Comparison ---")
    print(f"- Overall Agreement: {same_strategy_count}/{num_problems} problems ({strategy_agreement_perc:.2f}%).")
    
    print("  - Agreement by Difficulty:")
    for difficulty, stats in difficulty_stats.items():
        if stats['total'] > 0:
            agreement_perc = (stats['agreed'] / stats['total']) * 100
            print(f"    - {difficulty:<7}: {agreement_perc:.2f}% ({stats['agreed']}/{stats['total']})")
        else:
            print(f"    - {difficulty:<7}: No problems with this difficulty.")

    print("\n-----------------------------------------------------\n")


if __name__ == "__main__":
    main()
